{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5334fe15-e92f-48ca-8797-d593cbe52da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Database connection\n",
    "conn = sqlite3.connect('group_project.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create tables\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS City (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    name TEXT UNIQUE\n",
    ")''')\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS StateZip (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    code TEXT UNIQUE\n",
    ")''')\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS Housing (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    date TEXT,\n",
    "    price REAL,\n",
    "    bedrooms INTEGER,\n",
    "    bathrooms REAL,\n",
    "    sqft_living INTEGER,\n",
    "    sqft_lot INTEGER,\n",
    "    floors REAL,\n",
    "    waterfront INTEGER,\n",
    "    view INTEGER,\n",
    "    condition INTEGER,\n",
    "    sqft_above INTEGER,\n",
    "    sqft_basement INTEGER,\n",
    "    yr_built INTEGER,\n",
    "    yr_renovated INTEGER,\n",
    "    city_id INTEGER,\n",
    "    statezip_id INTEGER,\n",
    "    country TEXT,\n",
    "    FOREIGN KEY (city_id) REFERENCES City(id),\n",
    "    FOREIGN KEY (statezip_id) REFERENCES StateZip(id)\n",
    ")''')\n",
    "\n",
    "\n",
    "conn.commit()\n",
    "\n",
    "# Function to insert and obtain foreign key\n",
    "def get_or_create_fk(cursor, table, column, value):\n",
    "    cursor.execute(f'SELECT id FROM {table} WHERE {column} = ?', (value,))\n",
    "    result = cursor.fetchone()\n",
    "    if result:\n",
    "        return result[0]\n",
    "    else:\n",
    "        cursor.execute(f'INSERT INTO {table} ({column}) VALUES (?)', (value,))\n",
    "        conn.commit()\n",
    "        return cursor.lastrowid\n",
    "\n",
    "# Read and insert data\n",
    "with open('property.csv', 'r') as file:\n",
    "    next(file)  \n",
    "    for line in file:\n",
    "        row = line.strip().split(',')\n",
    "        if len(row) < 18:  \n",
    "            continue\n",
    "        city_id = get_or_create_fk(cursor, 'City', 'name', row[15])\n",
    "        statezip_id = get_or_create_fk(cursor, 'StateZip', 'code', row[16])\n",
    "        housing_data = row[:15] + [city_id, statezip_id, row[17]]\n",
    "        cursor.execute('''\n",
    "        INSERT INTO Housing (\n",
    "            date, price, bedrooms, bathrooms, sqft_living, sqft_lot, floors,\n",
    "            waterfront, view, condition, sqft_above, sqft_basement, yr_built,\n",
    "            yr_renovated, city_id, statezip_id, country\n",
    "        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        ''', group_project)\n",
    "\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e35e22fb-68db-4e41-9679-c790fc856e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ydata_profiling import ProfileReport\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler  # Import StandardScaler\n",
    "\n",
    "\n",
    "conn = sqlite3.connect('group_project.db')\n",
    "\n",
    "\n",
    "query = '''\n",
    "        SELECT h.date,h.price, h.bedrooms, h.bathrooms, h.sqft_living, h.sqft_lot, h.floors,\n",
    "        h.waterfront, h.view, h.condition, h.sqft_above, h.sqft_basement, h.yr_built,\n",
    "        h.yr_renovated, h.country, c.name AS city, s.code AS statezip\n",
    "        FROM  housing h\n",
    "        JOIN city c ON h.city_id = c.id\n",
    "        JOIN statezip s ON h.statezip_id = s.id;\n",
    "'''\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ef82207-3bad-4fee-9aa3-965e2224a846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>statezip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-05-02 00:00:00</td>\n",
       "      <td>3.130000e+05</td>\n",
       "      <td>3</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1340</td>\n",
       "      <td>7912</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1340</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>2005</td>\n",
       "      <td>USA</td>\n",
       "      <td>Shoreline</td>\n",
       "      <td>WA 98133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014-05-02 00:00:00</td>\n",
       "      <td>2.384000e+06</td>\n",
       "      <td>5</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3650</td>\n",
       "      <td>9050</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3370</td>\n",
       "      <td>280</td>\n",
       "      <td>1921</td>\n",
       "      <td>0</td>\n",
       "      <td>USA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA 98119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014-05-02 00:00:00</td>\n",
       "      <td>3.420000e+05</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1930</td>\n",
       "      <td>11947</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1930</td>\n",
       "      <td>0</td>\n",
       "      <td>1966</td>\n",
       "      <td>0</td>\n",
       "      <td>USA</td>\n",
       "      <td>Kent</td>\n",
       "      <td>WA 98042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014-05-02 00:00:00</td>\n",
       "      <td>4.200000e+05</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2000</td>\n",
       "      <td>8030</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1963</td>\n",
       "      <td>0</td>\n",
       "      <td>USA</td>\n",
       "      <td>Bellevue</td>\n",
       "      <td>WA 98008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-05-02 00:00:00</td>\n",
       "      <td>5.500000e+05</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1940</td>\n",
       "      <td>10500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1140</td>\n",
       "      <td>800</td>\n",
       "      <td>1976</td>\n",
       "      <td>1992</td>\n",
       "      <td>USA</td>\n",
       "      <td>Redmond</td>\n",
       "      <td>WA 98052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9195</th>\n",
       "      <td>2014-07-09 00:00:00</td>\n",
       "      <td>3.081667e+05</td>\n",
       "      <td>3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1510</td>\n",
       "      <td>6360</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1510</td>\n",
       "      <td>0</td>\n",
       "      <td>1954</td>\n",
       "      <td>1979</td>\n",
       "      <td>USA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA 98133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9196</th>\n",
       "      <td>2014-07-09 00:00:00</td>\n",
       "      <td>5.343333e+05</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1460</td>\n",
       "      <td>7573</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1460</td>\n",
       "      <td>0</td>\n",
       "      <td>1983</td>\n",
       "      <td>2009</td>\n",
       "      <td>USA</td>\n",
       "      <td>Bellevue</td>\n",
       "      <td>WA 98007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9197</th>\n",
       "      <td>2014-07-09 00:00:00</td>\n",
       "      <td>4.169042e+05</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>3010</td>\n",
       "      <td>7014</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3010</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>USA</td>\n",
       "      <td>Renton</td>\n",
       "      <td>WA 98059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9198</th>\n",
       "      <td>2014-07-10 00:00:00</td>\n",
       "      <td>2.034000e+05</td>\n",
       "      <td>4</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2090</td>\n",
       "      <td>6630</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1070</td>\n",
       "      <td>1020</td>\n",
       "      <td>1974</td>\n",
       "      <td>0</td>\n",
       "      <td>USA</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>WA 98178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9199</th>\n",
       "      <td>2014-07-10 00:00:00</td>\n",
       "      <td>2.206000e+05</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1490</td>\n",
       "      <td>8102</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1490</td>\n",
       "      <td>0</td>\n",
       "      <td>1990</td>\n",
       "      <td>0</td>\n",
       "      <td>USA</td>\n",
       "      <td>Covington</td>\n",
       "      <td>WA 98042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9200 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     date         price  bedrooms  bathrooms  sqft_living  \\\n",
       "0     2014-05-02 00:00:00  3.130000e+05         3       1.50         1340   \n",
       "1     2014-05-02 00:00:00  2.384000e+06         5       2.50         3650   \n",
       "2     2014-05-02 00:00:00  3.420000e+05         3       2.00         1930   \n",
       "3     2014-05-02 00:00:00  4.200000e+05         3       2.25         2000   \n",
       "4     2014-05-02 00:00:00  5.500000e+05         4       2.50         1940   \n",
       "...                   ...           ...       ...        ...          ...   \n",
       "9195  2014-07-09 00:00:00  3.081667e+05         3       1.75         1510   \n",
       "9196  2014-07-09 00:00:00  5.343333e+05         3       2.50         1460   \n",
       "9197  2014-07-09 00:00:00  4.169042e+05         3       2.50         3010   \n",
       "9198  2014-07-10 00:00:00  2.034000e+05         4       2.00         2090   \n",
       "9199  2014-07-10 00:00:00  2.206000e+05         3       2.50         1490   \n",
       "\n",
       "      sqft_lot  floors  waterfront  view  condition  sqft_above  \\\n",
       "0         7912     1.5           0     0          3        1340   \n",
       "1         9050     2.0           0     4          5        3370   \n",
       "2        11947     1.0           0     0          4        1930   \n",
       "3         8030     1.0           0     0          4        1000   \n",
       "4        10500     1.0           0     0          4        1140   \n",
       "...        ...     ...         ...   ...        ...         ...   \n",
       "9195      6360     1.0           0     0          4        1510   \n",
       "9196      7573     2.0           0     0          3        1460   \n",
       "9197      7014     2.0           0     0          3        3010   \n",
       "9198      6630     1.0           0     0          3        1070   \n",
       "9199      8102     2.0           0     0          4        1490   \n",
       "\n",
       "      sqft_basement  yr_built  yr_renovated country       city  statezip  \n",
       "0                 0      1955          2005     USA  Shoreline  WA 98133  \n",
       "1               280      1921             0     USA    Seattle  WA 98119  \n",
       "2                 0      1966             0     USA       Kent  WA 98042  \n",
       "3              1000      1963             0     USA   Bellevue  WA 98008  \n",
       "4               800      1976          1992     USA    Redmond  WA 98052  \n",
       "...             ...       ...           ...     ...        ...       ...  \n",
       "9195              0      1954          1979     USA    Seattle  WA 98133  \n",
       "9196              0      1983          2009     USA   Bellevue  WA 98007  \n",
       "9197              0      2009             0     USA     Renton  WA 98059  \n",
       "9198           1020      1974             0     USA    Seattle  WA 98178  \n",
       "9199              0      1990             0     USA  Covington  WA 98042  \n",
       "\n",
       "[9200 rows x 17 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "47bf1d43-65ab-42a1-bc20-1809b8e96a31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "332e900957bb4b1ba62194303be45934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd54d087e4f4891b78952e0c215f66d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "profile = ProfileReport(df, title=\"Profiling Report\")\n",
    "profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d4e6f2-50af-4fc9-bb5c-8416a89e40f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "for column in train.columns:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(train[column], kde=True)\n",
    "    plt.title(f'Distribution of {column}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "28a9d152-28ce-4474-a587-a35110184c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42, stratify=pd.qcut(df['price'], q=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "de9024e5-c2a9-4890-a4cc-1c60934efe49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Shoreline' 'Seattle' 'Kent' 'Bellevue' 'Redmond' 'Maple Valley'\n",
      " 'North Bend' 'Lake Forest Park' 'Sammamish' 'Auburn' 'Des Moines'\n",
      " 'Bothell' 'Federal Way' 'Kirkland' 'Issaquah' 'Woodinville'\n",
      " 'Normandy Park' 'Fall City' 'Renton' 'Carnation' 'Snoqualmie' 'Duvall'\n",
      " 'Burien' 'Covington' 'Inglewood-Finn Hill' 'Kenmore' 'Newcastle'\n",
      " 'Mercer Island' 'Black Diamond' 'Ravensdale' 'Clyde Hill' 'Algona'\n",
      " 'Skykomish' 'Tukwila' 'Vashon' 'Yarrow Point' 'SeaTac' 'Medina'\n",
      " 'Enumclaw' 'Snoqualmie Pass' 'Pacific' 'Beaux Arts Village' 'Preston'\n",
      " 'Milton']\n",
      "object\n",
      "            date      price  bedrooms  bathrooms  sqft_living  sqft_lot  \\\n",
      "0  5/2/2014 0:00   313000.0         3       1.50         1340      7912   \n",
      "1  5/2/2014 0:00  2384000.0         5       2.50         3650      9050   \n",
      "2  5/2/2014 0:00   342000.0         3       2.00         1930     11947   \n",
      "3  5/2/2014 0:00   420000.0         3       2.25         2000      8030   \n",
      "4  5/2/2014 0:00   550000.0         4       2.50         1940     10500   \n",
      "\n",
      "   floors  waterfront  view  condition  ...  city_SeaTac  city_Seattle  \\\n",
      "0     1.5           0     0          3  ...          0.0           0.0   \n",
      "1     2.0           0     4          5  ...          0.0           1.0   \n",
      "2     1.0           0     0          4  ...          0.0           0.0   \n",
      "3     1.0           0     0          4  ...          0.0           0.0   \n",
      "4     1.0           0     0          4  ...          0.0           0.0   \n",
      "\n",
      "   city_Shoreline  city_Skykomish city_Snoqualmie  city_Snoqualmie Pass  \\\n",
      "0             1.0             0.0             0.0                   0.0   \n",
      "1             0.0             0.0             0.0                   0.0   \n",
      "2             0.0             0.0             0.0                   0.0   \n",
      "3             0.0             0.0             0.0                   0.0   \n",
      "4             0.0             0.0             0.0                   0.0   \n",
      "\n",
      "   city_Tukwila  city_Vashon  city_Woodinville  city_Yarrow Point  \n",
      "0           0.0          0.0               0.0                0.0  \n",
      "1           0.0          0.0               0.0                0.0  \n",
      "2           0.0          0.0               0.0                0.0  \n",
      "3           0.0          0.0               0.0                0.0  \n",
      "4           0.0          0.0               0.0                0.0  \n",
      "\n",
      "[5 rows x 59 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ideal\\AppData\\Local\\Temp\\ipykernel_37896\\1943669450.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['city'].fillna('Unknown', inplace=True)\n",
      "C:\\Users\\ideal\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('property.csv')\n",
    "\n",
    "# Inspect the unique values and data types in the 'street' column\n",
    "print(df['city'].unique())\n",
    "print(df['city'].dtype)\n",
    "\n",
    "# Handle missing values if any\n",
    "df['city'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "# Fit and transform the 'street' column\n",
    "city_encoded = onehot_encoder.fit_transform(df[['city']])\n",
    "\n",
    "# Create a DataFrame with the encoded features\n",
    "city_encoded_df = pd.DataFrame(city_encoded, columns=onehot_encoder.get_feature_names_out(['city']))\n",
    "\n",
    "# Concatenate the encoded features with the original DataFrame\n",
    "df = pd.concat([df, city_encoded_df], axis=1)\n",
    "\n",
    "# Drop the original 'street' column\n",
    "df.drop('city', axis=1, inplace=True)\n",
    "\n",
    "# Display the DataFrame with the encoded 'street' column\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "15e8d02c-91af-4c62-a8f3-08349a3f14c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['date'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[202], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m test\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m,  axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5446\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[0;32m   5582\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   5583\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   5584\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   5585\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   5586\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   5587\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[0;32m   5588\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   5589\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['date'] not found in axis\""
     ]
    }
   ],
   "source": [
    "train.drop('date', axis=1, inplace=True)\n",
    "test.drop('date',  axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "cd196782-bb77-49bc-9551-6711452ed814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n",
       "       'waterfront', 'view', 'condition', 'sqft_above', 'sqft_basement',\n",
       "       'yr_built', 'yr_renovated', 'city_Algona', 'city_Auburn',\n",
       "       'city_Beaux Arts Village', 'city_Bellevue', 'city_Black Diamond',\n",
       "       'city_Bothell', 'city_Burien', 'city_Carnation', 'city_Clyde Hill',\n",
       "       'city_Covington', 'city_Des Moines', 'city_Duvall', 'city_Enumclaw',\n",
       "       'city_Fall City', 'city_Federal Way', 'city_Inglewood-Finn Hill',\n",
       "       'city_Issaquah', 'city_Kenmore', 'city_Kent', 'city_Kirkland',\n",
       "       'city_Lake Forest Park', 'city_Maple Valley', 'city_Medina',\n",
       "       'city_Mercer Island', 'city_Milton', 'city_Newcastle',\n",
       "       'city_Normandy Park', 'city_North Bend', 'city_Pacific', 'city_Preston',\n",
       "       'city_Ravensdale', 'city_Redmond', 'city_Renton', 'city_Sammamish',\n",
       "       'city_SeaTac', 'city_Seattle', 'city_Shoreline', 'city_Skykomish',\n",
       "       'city_Snoqualmie', 'city_Snoqualmie Pass', 'city_Tukwila',\n",
       "       'city_Vashon', 'city_Woodinville', 'city_Yarrow Point'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e0b531f2-f5b6-4571-8ed8-8ce42b3d4ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n",
       "       'waterfront', 'view', 'condition', 'sqft_above', 'sqft_basement',\n",
       "       'yr_built', 'yr_renovated', 'city_Algona', 'city_Auburn',\n",
       "       'city_Beaux Arts Village', 'city_Bellevue', 'city_Black Diamond',\n",
       "       'city_Bothell', 'city_Burien', 'city_Carnation', 'city_Clyde Hill',\n",
       "       'city_Covington', 'city_Des Moines', 'city_Duvall', 'city_Enumclaw',\n",
       "       'city_Fall City', 'city_Federal Way', 'city_Inglewood-Finn Hill',\n",
       "       'city_Issaquah', 'city_Kenmore', 'city_Kent', 'city_Kirkland',\n",
       "       'city_Lake Forest Park', 'city_Maple Valley', 'city_Medina',\n",
       "       'city_Mercer Island', 'city_Milton', 'city_Newcastle',\n",
       "       'city_Normandy Park', 'city_North Bend', 'city_Pacific', 'city_Preston',\n",
       "       'city_Ravensdale', 'city_Redmond', 'city_Renton', 'city_Sammamish',\n",
       "       'city_SeaTac', 'city_Seattle', 'city_Shoreline', 'city_Skykomish',\n",
       "       'city_Snoqualmie', 'city_Snoqualmie Pass', 'city_Tukwila',\n",
       "       'city_Vashon', 'city_Woodinville', 'city_Yarrow Point'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "94b03265-eadd-479e-8d8d-4e810f5cffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DataPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    categorical_columns = train.select_dtypes(include=['object']).columns\n",
    "    numerical_columns = train.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "    def fit(self, X, y=None): \n",
    "        # Create and fit simple imputer\n",
    "        self.imputer = SimpleImputer(strategy='median')\n",
    "        self.imputer.fit(X[self.numerical_columns])\n",
    "        \n",
    "        # Create and fit Standard Scaler \n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit(X[self.numerical_columns]) \n",
    "        \n",
    "        # Create and fit one hot encoder\n",
    "        self.onehot = OneHotEncoder(handle_unknown='ignore')\n",
    "        self.onehot.fit(X[self.categorical_columns])\n",
    "        \n",
    "        return self \n",
    " \n",
    "\n",
    "    def transform(self, X): \n",
    "        # Apply simple imputer \n",
    "        imputed_cols = self.imputer.transform(X[self.numerical_columns])\n",
    "        onehot_cols = self.onehot.transform(X[self.categorical_columns])\n",
    "        \n",
    "        # Copy the df \n",
    "        transformed_df = X.copy()\n",
    "         \n",
    "        # Apply transformed columns\n",
    "        transformed_df[self.numerical_columns] = imputed_cols\n",
    "        transformed_df[self.numerical_columns] = self.scaler.transform(transformed_df[self.numerical_columns])        \n",
    "        \n",
    "        # Drop existing categorical columns and replace with one hot equivalent\n",
    "        transformed_df = transformed_df.drop(self.categorical_columns, axis=1) \n",
    "        transformed_df[self.onehot.get_feature_names_out()] = onehot_cols.toarray().astype(int)\n",
    "        \n",
    "        return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "d45f7c37-1229-47c1-92f7-bbf6d620a303",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['date', 'country'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[200], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m DataPreprocessor()\n\u001b[1;32m----> 2\u001b[0m preprocessor\u001b[38;5;241m.\u001b[39mfit(train)\n\u001b[0;32m      3\u001b[0m train_fixed \u001b[38;5;241m=\u001b[39m preprocessor\u001b[38;5;241m.\u001b[39mtransform(train)\n",
      "Cell \u001b[1;32mIn[150], line 20\u001b[0m, in \u001b[0;36mDataPreprocessor.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Create and fit one hot encoder\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monehot \u001b[38;5;241m=\u001b[39m OneHotEncoder(handle_unknown\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monehot\u001b[38;5;241m.\u001b[39mfit(X[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategorical_columns])\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['date', 'country'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "preprocessor = DataPreprocessor()\n",
    "preprocessor.fit(train)\n",
    "train_fixed = preprocessor.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "37cfff65-2162-4ac4-af61-be4552a30561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfr = make_pipeline(DataPreprocessor(), RandomForestRegressor(n_estimators=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "01eb0a12-49ae-42d1-8035-2cc5d858ae51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('datapreprocessor', DataPreprocessor()),\n",
       "  ('randomforestregressor', RandomForestRegressor(n_estimators=50))],\n",
       " 'verbose': False,\n",
       " 'datapreprocessor': DataPreprocessor(),\n",
       " 'randomforestregressor': RandomForestRegressor(n_estimators=50),\n",
       " 'randomforestregressor__bootstrap': True,\n",
       " 'randomforestregressor__ccp_alpha': 0.0,\n",
       " 'randomforestregressor__criterion': 'squared_error',\n",
       " 'randomforestregressor__max_depth': None,\n",
       " 'randomforestregressor__max_features': 1.0,\n",
       " 'randomforestregressor__max_leaf_nodes': None,\n",
       " 'randomforestregressor__max_samples': None,\n",
       " 'randomforestregressor__min_impurity_decrease': 0.0,\n",
       " 'randomforestregressor__min_samples_leaf': 1,\n",
       " 'randomforestregressor__min_samples_split': 2,\n",
       " 'randomforestregressor__min_weight_fraction_leaf': 0.0,\n",
       " 'randomforestregressor__n_estimators': 50,\n",
       " 'randomforestregressor__n_jobs': None,\n",
       " 'randomforestregressor__oob_score': False,\n",
       " 'randomforestregressor__random_state': None,\n",
       " 'randomforestregressor__verbose': 0,\n",
       " 'randomforestregressor__warm_start': False}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = rfr.get_params()\n",
    "params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "7f292d0a-050a-4aba-be90-f7a7cc8fbe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train[\"price\"]\n",
    "X_train = train.drop(\"price\",axis=1)\n",
    "y_test = test['price']\n",
    "X_test = test.drop('price', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "2c376f2d-ce23-4776-bc7b-b48abdeb2f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr = RandomForestRegressor()\n",
    "rfr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "1332055f-7801-4b6d-b66e-866516117a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "y_train_hat=rfr.predict(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "87dc9785-33ed-49d9-b3d8-7070f561cb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = mean_squared_error(y_train, y_train_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "6889c358-2eeb-42d1-a0b8-58d791c6fd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(y_train, y_train_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "2734773f-ed13-4845-99e5-bebce274ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "y_test_hat=rfr.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a72fdf8a-18c4-4e67-88d3-6b397eaa6c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263852.01278111385"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, y_test_hat,squared=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "f43703d5-b369-48f6-be43-d824614ec4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = r2_score(y_train, y_train_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "61ede448-0da5-4c38-a16b-2931097aa3c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127796.55544685041"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y_test, y_test_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "64a23041-a00e-4726-89a5-8bf5dd9d7d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "3385cd1f-48fd-43f4-ad20-cd7af0e76654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4497157723498919"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test, y_test_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0bef43f9-af4d-49e0-b6c8-5b5431f3a015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n",
       "       'waterfront', 'view', 'condition', 'sqft_above', 'sqft_basement',\n",
       "       'yr_built', 'yr_renovated', 'country', 'city_Algona', 'city_Auburn',\n",
       "       'city_Beaux Arts Village', 'city_Bellevue', 'city_Black Diamond',\n",
       "       'city_Bothell', 'city_Burien', 'city_Carnation', 'city_Clyde Hill',\n",
       "       'city_Covington', 'city_Des Moines', 'city_Duvall', 'city_Enumclaw',\n",
       "       'city_Fall City', 'city_Federal Way', 'city_Inglewood-Finn Hill',\n",
       "       'city_Issaquah', 'city_Kenmore', 'city_Kent', 'city_Kirkland',\n",
       "       'city_Lake Forest Park', 'city_Maple Valley', 'city_Medina',\n",
       "       'city_Mercer Island', 'city_Milton', 'city_Newcastle',\n",
       "       'city_Normandy Park', 'city_North Bend', 'city_Pacific', 'city_Preston',\n",
       "       'city_Ravensdale', 'city_Redmond', 'city_Renton', 'city_Sammamish',\n",
       "       'city_SeaTac', 'city_Seattle', 'city_Shoreline', 'city_Skykomish',\n",
       "       'city_Snoqualmie', 'city_Snoqualmie Pass', 'city_Tukwila',\n",
       "       'city_Vashon', 'city_Woodinville', 'city_Yarrow Point'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3d19a17b-d3d0-4d62-957f-8db8ce78f536",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop('country', axis=1, inplace=True)\n",
    "test.drop('country',  axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "0c823dd2-6420-4c15-a86c-c9035cebb161",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_37896\\3273540463.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6295\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6296\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6297\u001b[0m         ):\n\u001b[0;32m   6298\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6299\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "y_train.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "230ee1c4-6386-4383-a785-89bd6af3789d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ideal\\anaconda3\\Lib\\site-packages\\mlflow\\types\\utils.py:394: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ideal\\anaconda3\\Lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Successfully registered model 'rfr_housing_model'.\n",
      "2024/05/15 22:59:02 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: rfr_housing_model, version 1\n",
      "Created version '1' of model 'rfr_housing_model'.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "# Set our tracking server uri for logging\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")\n",
    "\n",
    "# Create a new MLflow Experiment\n",
    "mlflow.set_experiment(\"Housing Price Prediction\")\n",
    "experiment_name = \"RANDOM_FOREST_REGRESSI0N\"\n",
    "try:\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "except MlflowException as e:\n",
    "    if \"RESOURCE_DOES_NOT_EXIST\" in str(e):\n",
    "        mlflow.create_experiment(experiment_name)\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "    else:\n",
    "        raise e\n",
    "# Start an MLflow run\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Log the hyperparameters\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"root_mean_squared_error\", rmse)\n",
    "    mlflow.log_metric(\"mean_absolute_error\", mae)\n",
    "    mlflow.log_metric(\"r2_score\", r2)\n",
    "\n",
    "    # Set a tag that we can use to remind ourselves what this run was for\n",
    "    mlflow.set_tag(\"Model Info\", \"RandomForestRegressor for housing data\")\n",
    "\n",
    "    # Infer the model signature\n",
    "    signature = infer_signature(X_train, model.predict(X_train))\n",
    "\n",
    "    # Log the model\n",
    "    model_info = mlflow.sklearn.log_model(\n",
    "        sk_model=model,\n",
    "        artifact_path=\"housing_model\",\n",
    "        signature=signature,\n",
    "        registered_model_name=\"rfr_housing_model\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "d1717a2e-cd15-4e97-881f-c5caebe5d404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.0.3-py3-none-win_amd64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\ideal\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\ideal\\anaconda3\\lib\\site-packages (from xgboost) (1.11.4)\n",
      "Downloading xgboost-2.0.3-py3-none-win_amd64.whl (99.8 MB)\n",
      "   ---------------------------------------- 0.0/99.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/99.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/99.8 MB 217.9 kB/s eta 0:07:38\n",
      "   ---------------------------------------- 0.1/99.8 MB 469.7 kB/s eta 0:03:33\n",
      "   ---------------------------------------- 0.3/99.8 MB 1.9 MB/s eta 0:00:53\n",
      "   ---------------------------------------- 1.2/99.8 MB 5.2 MB/s eta 0:00:19\n",
      "   - -------------------------------------- 2.5/99.8 MB 10.0 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 4.1/99.8 MB 13.8 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 6.0/99.8 MB 17.5 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 8.1/99.8 MB 20.0 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 10.0/99.8 MB 21.9 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 11.8/99.8 MB 38.5 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 14.2/99.8 MB 40.9 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 15.4/99.8 MB 40.9 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 17.6/99.8 MB 43.7 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 19.2/99.8 MB 43.7 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 22.0/99.8 MB 46.7 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 23.6/99.8 MB 43.5 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 25.2/99.8 MB 40.9 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 27.0/99.8 MB 40.9 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 28.8/99.8 MB 40.9 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 30.2/99.8 MB 38.6 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 32.1/99.8 MB 36.4 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 33.4/99.8 MB 34.4 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 35.1/99.8 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 36.4/99.8 MB 36.4 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 38.2/99.8 MB 34.4 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 40.2/99.8 MB 34.6 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 42.2/99.8 MB 38.5 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 44.2/99.8 MB 38.6 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 46.0/99.8 MB 40.9 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 47.7/99.8 MB 38.5 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 48.9/99.8 MB 38.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 50.7/99.8 MB 38.5 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 52.7/99.8 MB 38.5 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 54.2/99.8 MB 36.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 56.3/99.8 MB 38.5 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 57.9/99.8 MB 38.5 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 59.7/99.8 MB 38.6 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 60.8/99.8 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 62.1/99.8 MB 34.4 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 63.6/99.8 MB 32.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 65.0/99.8 MB 32.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 66.0/99.8 MB 32.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 67.1/99.8 MB 31.2 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 68.2/99.8 MB 28.5 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 69.3/99.8 MB 26.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 70.2/99.8 MB 27.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 70.8/99.8 MB 24.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 71.6/99.8 MB 24.3 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 72.9/99.8 MB 23.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 73.4/99.8 MB 23.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 73.9/99.8 MB 20.5 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 74.9/99.8 MB 19.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 75.8/99.8 MB 19.3 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 77.0/99.8 MB 19.8 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 77.9/99.8 MB 19.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 79.1/99.8 MB 19.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 80.4/99.8 MB 18.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 80.4/99.8 MB 18.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 81.1/99.8 MB 18.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 82.6/99.8 MB 19.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 84.1/99.8 MB 21.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 85.5/99.8 MB 23.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 87.4/99.8 MB 24.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 89.2/99.8 MB 27.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 91.1/99.8 MB 32.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 92.7/99.8 MB 34.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 93.8/99.8 MB 34.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 95.6/99.8 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  97.4/99.8 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  98.4/99.8 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 99.8/99.8 MB 14.9 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "f4045c16-5797-4b20-8171-a8c951283cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ideal\\anaconda3\\Lib\\site-packages\\mlflow\\types\\utils.py:394: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ideal\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [23:26:16] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\c_api\\c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "Successfully registered model 'xgb_housing_model'.\n",
      "2024/05/15 23:26:20 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: xgb_housing_model, version 1\n",
      "Created version '1' of model 'xgb_housing_model'.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Set our tracking server uri for logging\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")\n",
    "\n",
    "# Create a new MLflow Experiment\n",
    "experiment_name = \"Housing Price Prediction\"\n",
    "try:\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "except mlflow.exceptions.MlflowException as e:\n",
    "    if \"RESOURCE_DOES_NOT_EXIST\" in str(e):\n",
    "        mlflow.create_experiment(experiment_name)\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "# Define the model\n",
    "model = XGBRegressor()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    # Log the hyperparameters\n",
    "    mlflow.log_params(model.get_params())\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate and log metrics\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    mlflow.log_metric(\"root_mean_squared_error\", rmse)\n",
    "    mlflow.log_metric(\"mean_absolute_error\", mae)\n",
    "    mlflow.log_metric(\"r2_score\", r2)\n",
    "\n",
    "    # Set a tag that we can use to remind ourselves what this run was for\n",
    "    mlflow.set_tag(\"Model Info\", \"XGBRegressor for housing data\")\n",
    "\n",
    "    # Infer the model signature\n",
    "    signature = infer_signature(X_train, model.predict(X_train))\n",
    "\n",
    "    # Log the model\n",
    "    model_info = mlflow.xgboost.log_model(\n",
    "        xgb_model=model,\n",
    "        artifact_path=\"housing_model\",\n",
    "        signature=signature,\n",
    "        registered_model_name=\"xgb_housing_model\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "d8df409b-c1f2-40fd-87f0-f6d790fef5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ideal\\anaconda3\\Lib\\site-packages\\mlflow\\types\\utils.py:394: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "Successfully registered model 'bagging_housing_model'.\n",
      "2024/05/15 23:40:13 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: bagging_housing_model, version 1\n",
      "Created version '1' of model 'bagging_housing_model'.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Set our tracking server uri for logging\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")\n",
    "\n",
    "# Create a new MLflow Experiment\n",
    "experiment_name = \"Housing Price Prediction\"\n",
    "try:\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "except mlflow.exceptions.MlflowException as e:\n",
    "    if \"RESOURCE_DOES_NOT_EXIST\" in str(e):\n",
    "        mlflow.create_experiment(experiment_name)\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "# Define the model using BaggingRegressor\n",
    "model = BaggingRegressor()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    # Log the hyperparameters\n",
    "    mlflow.log_params(model.get_params())\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate and log metrics\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    mlflow.log_metric(\"root_mean_squared_error\", rmse)\n",
    "    mlflow.log_metric(\"mean_absolute_error\", mae)\n",
    "    mlflow.log_metric(\"r2_score\", r2)\n",
    "\n",
    "    # Set a tag that we can use to remind ourselves what this run was for\n",
    "    mlflow.set_tag(\"Model Info\", \"BaggingRegressor for housing data\")\n",
    "\n",
    "    # Infer the model signature\n",
    "    signature = infer_signature(X_train, model.predict(X_train))\n",
    "\n",
    "    # Log the model\n",
    "    model_info = mlflow.sklearn.log_model(\n",
    "        sk_model=model,\n",
    "        artifact_path=\"housing_model\",\n",
    "        signature=signature,\n",
    "        registered_model_name=\"bagging_housing_model\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "a1f1acc8-a6ab-4f2a-a76d-068f434a3040",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop('view', axis=1, inplace=True)\n",
    "test.drop('view',  axis=1, inplace=True)\n",
    "train.drop('waterfront', axis=1, inplace=True)\n",
    "test.drop('waterfront',  axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "0a5a61aa-8069-4e7e-8230-5ac55489bbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ideal\\anaconda3\\Lib\\site-packages\\mlflow\\types\\utils.py:394: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "Successfully registered model 'gbr_housing_model'.\n",
      "2024/05/15 23:47:04 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: gbr_housing_model, version 1\n",
      "Created version '1' of model 'gbr_housing_model'.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Set our tracking server uri for logging\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")\n",
    "\n",
    "# Create a new MLflow Experiment\n",
    "experiment_name = \"Housing Price Prediction\"\n",
    "try:\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "except mlflow.exceptions.MlflowException as e:\n",
    "    if \"RESOURCE_DOES_NOT_EXIST\" in str(e):\n",
    "        mlflow.create_experiment(experiment_name)\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "# Define the model using GradientBoostingRegressor\n",
    "model = GradientBoostingRegressor()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    # Log the hyperparameters\n",
    "    mlflow.log_params(model.get_params())\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate and log metrics\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    mlflow.log_metric(\"root_mean_squared_error\", rmse)\n",
    "    mlflow.log_metric(\"mean_absolute_error\", mae)\n",
    "    mlflow.log_metric(\"r2_score\", r2)\n",
    "\n",
    "    # Set a tag that we can use to remind ourselves what this run was for\n",
    "    mlflow.set_tag(\"Model Info\", \"GradientBoostingRegressor for housing data\")\n",
    "\n",
    "    # Infer the model signature\n",
    "    signature = infer_signature(X_train, model.predict(X_train))\n",
    "\n",
    "    # Log the model\n",
    "    model_info = mlflow.sklearn.log_model(\n",
    "        sk_model=model,\n",
    "        artifact_path=\"housing_model\",\n",
    "        signature=signature,\n",
    "        registered_model_name=\"gbr_housing_model\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0f891b-5876-47b3-97e0-ed8b5c51efc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
