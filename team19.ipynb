{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5334fe15-e92f-48ca-8797-d593cbe52da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# create Database connection\n",
    "conn = sqlite3.connect('group_project.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create tables\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS City (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    name TEXT UNIQUE\n",
    ")''')\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS StateZip (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    code TEXT UNIQUE\n",
    ")''')\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS Housing (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    date TEXT,\n",
    "    price REAL,\n",
    "    bedrooms INTEGER,\n",
    "    bathrooms REAL,\n",
    "    sqft_living INTEGER,\n",
    "    sqft_lot INTEGER,\n",
    "    floors REAL,\n",
    "    waterfront INTEGER,\n",
    "    view INTEGER,\n",
    "    condition INTEGER,\n",
    "    sqft_above INTEGER,\n",
    "    sqft_basement INTEGER,\n",
    "    yr_built INTEGER,\n",
    "    yr_renovated INTEGER,\n",
    "    city_id INTEGER,\n",
    "    statezip_id INTEGER,\n",
    "    country TEXT,\n",
    "    FOREIGN KEY (city_id) REFERENCES City(id),\n",
    "    FOREIGN KEY (statezip_id) REFERENCES StateZip(id)\n",
    ")''')\n",
    "\n",
    "\n",
    "conn.commit()#commit the changes to the database\n",
    "\n",
    "# Function to insert and obtain foreign key\n",
    "def get_or_create_fk(cursor, table, column, value):\n",
    "    cursor.execute(f'SELECT id FROM {table} WHERE {column} = ?', (value,))\n",
    "    result = cursor.fetchone()\n",
    "    if result:\n",
    "        return result[0]\n",
    "    else:\n",
    "        cursor.execute(f'INSERT INTO {table} ({column}) VALUES (?)', (value,))\n",
    "        conn.commit()\n",
    "        return cursor.lastrowid\n",
    "\n",
    "# Read and insert data\n",
    "with open('property.csv', 'r') as file:\n",
    "    next(file)  \n",
    "    for line in file:\n",
    "        #print(line)\n",
    "        row = line.strip().split(',')\n",
    "        if len(row) < 18:  \n",
    "            continue\n",
    "        city_id = get_or_create_fk(cursor, 'City', 'name', row[15])\n",
    "        statezip_id = get_or_create_fk(cursor, 'StateZip', 'code', row[16])\n",
    "        housing_data = row[:15] + [city_id, statezip_id, row[17]]\n",
    "        cursor.execute('''\n",
    "        INSERT INTO Housing (\n",
    "            date, price, bedrooms, bathrooms, sqft_living, sqft_lot, floors,\n",
    "            waterfront, view, condition, sqft_above, sqft_basement, yr_built,\n",
    "            yr_renovated, city_id, statezip_id, country\n",
    "        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        ''', group_project)\n",
    "\n",
    "\n",
    "conn.commit()#commit the changes to the database\n",
    "conn.close()#close the connection to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54404585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ydata_profiling\n",
      "  Obtaining dependency information for ydata_profiling from https://files.pythonhosted.org/packages/33/85/45027914eb485482976883dcaab434eab99f1ed5cb222781e7ba46bdf3b7/ydata_profiling-4.8.3-py2.py3-none-any.whl.metadata\n",
      "  Downloading ydata_profiling-4.8.3-py2.py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: scipy<1.14,>=1.4.1 in /opt/anaconda3/lib/python3.9/site-packages (from ydata_profiling) (1.11.1)\n",
      "Requirement already satisfied: pandas!=1.4.0,<3,>1.1 in /opt/anaconda3/lib/python3.9/site-packages (from ydata_profiling) (2.0.3)\n",
      "Requirement already satisfied: matplotlib<3.9,>=3.2 in /opt/anaconda3/lib/python3.9/site-packages (from ydata_profiling) (3.7.2)\n",
      "Collecting pydantic>=2 (from ydata_profiling)\n",
      "  Obtaining dependency information for pydantic>=2 from https://files.pythonhosted.org/packages/ed/76/9a17032880ed27f2dbd490c77a3431cbc80f47ba81534131de3c2846e736/pydantic-2.7.1-py3-none-any.whl.metadata\n",
      "  Downloading pydantic-2.7.1-py3-none-any.whl.metadata (107 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.3/107.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: PyYAML<6.1,>=5.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from ydata_profiling) (6.0)\n",
      "Requirement already satisfied: jinja2<3.2,>=2.11.1 in /opt/anaconda3/lib/python3.9/site-packages (from ydata_profiling) (3.1.2)\n",
      "Collecting visions[type_image_path]<0.7.7,>=0.7.5 (from ydata_profiling)\n",
      "  Obtaining dependency information for visions[type_image_path]<0.7.7,>=0.7.5 from https://files.pythonhosted.org/packages/7c/bf/612b24e711ae25dea9af19b9304634b8949faa0b035fad47e8bcadf62f59/visions-0.7.6-py3-none-any.whl.metadata\n",
      "  Downloading visions-0.7.6-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2,>=1.16.0 in /opt/anaconda3/lib/python3.9/site-packages (from ydata_profiling) (1.24.3)\n",
      "Collecting htmlmin==0.1.12 (from ydata_profiling)\n",
      "  Downloading htmlmin-0.1.12.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting phik<0.13,>=0.11.1 (from ydata_profiling)\n",
      "  Obtaining dependency information for phik<0.13,>=0.11.1 from https://files.pythonhosted.org/packages/76/7c/8cf317aca8477994318ddfcffd222e8be01b5bd9065aecf6a191da848518/phik-0.12.4-cp39-cp39-macosx_10_13_x86_64.whl.metadata\n",
      "  Downloading phik-0.12.4-cp39-cp39-macosx_10_13_x86_64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: requests<3,>=2.24.0 in /opt/anaconda3/lib/python3.9/site-packages (from ydata_profiling) (2.31.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.48.2 in /opt/anaconda3/lib/python3.9/site-packages (from ydata_profiling) (4.65.0)\n",
      "Requirement already satisfied: seaborn<0.14,>=0.10.1 in /opt/anaconda3/lib/python3.9/site-packages (from ydata_profiling) (0.12.2)\n",
      "Collecting multimethod<2,>=1.4 (from ydata_profiling)\n",
      "  Obtaining dependency information for multimethod<2,>=1.4 from https://files.pythonhosted.org/packages/a0/96/47dc456936530adb1360aba7300f2da2e1d277fb361e025db3926653e189/multimethod-1.11.2-py3-none-any.whl.metadata\n",
      "  Downloading multimethod-1.11.2-py3-none-any.whl.metadata (9.1 kB)\n",
      "Requirement already satisfied: statsmodels<1,>=0.13.2 in /opt/anaconda3/lib/python3.9/site-packages (from ydata_profiling) (0.14.0)\n",
      "Collecting typeguard<5,>=3 (from ydata_profiling)\n",
      "  Obtaining dependency information for typeguard<5,>=3 from https://files.pythonhosted.org/packages/d9/59/e02336eb478ccdfc9bb0d4c27ce04a4260cd8b45aa04f6b00bcfdbb66a2a/typeguard-4.2.1-py3-none-any.whl.metadata\n",
      "  Downloading typeguard-4.2.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting imagehash==4.3.1 (from ydata_profiling)\n",
      "  Obtaining dependency information for imagehash==4.3.1 from https://files.pythonhosted.org/packages/2d/b4/19a746a986c6e38595fa5947c028b1b8e287773dcad766e648897ad2a4cf/ImageHash-4.3.1-py2.py3-none-any.whl.metadata\n",
      "  Downloading ImageHash-4.3.1-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting wordcloud>=1.9.1 (from ydata_profiling)\n",
      "  Obtaining dependency information for wordcloud>=1.9.1 from https://files.pythonhosted.org/packages/07/18/9e2bc9d5ee2c88514f368c8ccc82aae4f07392ccda41dc1706fe4cf52a0e/wordcloud-1.9.3-cp39-cp39-macosx_10_9_x86_64.whl.metadata\n",
      "  Downloading wordcloud-1.9.3-cp39-cp39-macosx_10_9_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting dacite>=1.8 (from ydata_profiling)\n",
      "  Obtaining dependency information for dacite>=1.8 from https://files.pythonhosted.org/packages/21/0f/cf0943f4f55f0fbc7c6bd60caf1343061dff818b02af5a0d444e473bb78d/dacite-1.8.1-py3-none-any.whl.metadata\n",
      "  Downloading dacite-1.8.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numba<1,>=0.56.0 in /opt/anaconda3/lib/python3.9/site-packages (from ydata_profiling) (0.57.0)\n",
      "Requirement already satisfied: PyWavelets in /opt/anaconda3/lib/python3.9/site-packages (from imagehash==4.3.1->ydata_profiling) (1.4.1)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.9/site-packages (from imagehash==4.3.1->ydata_profiling) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.9/site-packages (from jinja2<3.2,>=2.11.1->ydata_profiling) (2.1.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib<3.9,>=3.2->ydata_profiling) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib<3.9,>=3.2->ydata_profiling) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib<3.9,>=3.2->ydata_profiling) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib<3.9,>=3.2->ydata_profiling) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib<3.9,>=3.2->ydata_profiling) (23.1)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib<3.9,>=3.2->ydata_profiling) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib<3.9,>=3.2->ydata_profiling) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib<3.9,>=3.2->ydata_profiling) (5.2.0)\n",
      "Requirement already satisfied: llvmlite<0.41,>=0.40.0dev0 in /opt/anaconda3/lib/python3.9/site-packages (from numba<1,>=0.56.0->ydata_profiling) (0.40.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.9/site-packages (from pandas!=1.4.0,<3,>1.1->ydata_profiling) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.9/site-packages (from pandas!=1.4.0,<3,>1.1->ydata_profiling) (2023.3)\n",
      "Requirement already satisfied: joblib>=0.14.1 in /opt/anaconda3/lib/python3.9/site-packages (from phik<0.13,>=0.11.1->ydata_profiling) (1.2.0)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic>=2->ydata_profiling)\n",
      "  Obtaining dependency information for annotated-types>=0.4.0 from https://files.pythonhosted.org/packages/28/78/d31230046e58c207284c6b2c4e8d96e6d3cb4e52354721b944d3e1ee4aa5/annotated_types-0.6.0-py3-none-any.whl.metadata\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.18.2 (from pydantic>=2->ydata_profiling)\n",
      "  Obtaining dependency information for pydantic-core==2.18.2 from https://files.pythonhosted.org/packages/29/d9/96651085f19d96ef180743fabf2341b72265ddc29a41c184cfe4b1f9e97f/pydantic_core-2.18.2-cp39-cp39-macosx_10_12_x86_64.whl.metadata\n",
      "  Downloading pydantic_core-2.18.2-cp39-cp39-macosx_10_12_x86_64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/anaconda3/lib/python3.9/site-packages (from pydantic>=2->ydata_profiling) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.24.0->ydata_profiling) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.24.0->ydata_profiling) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.24.0->ydata_profiling) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.24.0->ydata_profiling) (2023.7.22)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: patsy>=0.5.2 in /opt/anaconda3/lib/python3.9/site-packages (from statsmodels<1,>=0.13.2->ydata_profiling) (0.5.3)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /opt/anaconda3/lib/python3.9/site-packages (from typeguard<5,>=3->ydata_profiling) (6.0.0)\n",
      "Collecting typing-extensions>=4.6.1 (from pydantic>=2->ydata_profiling)\n",
      "  Obtaining dependency information for typing-extensions>=4.6.1 from https://files.pythonhosted.org/packages/01/f3/936e209267d6ef7510322191003885de524fc48d1b43269810cd589ceaf5/typing_extensions-4.11.0-py3-none-any.whl.metadata\n",
      "  Downloading typing_extensions-4.11.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: attrs>=19.3.0 in /opt/anaconda3/lib/python3.9/site-packages (from visions[type_image_path]<0.7.7,>=0.7.5->ydata_profiling) (22.1.0)\n",
      "Requirement already satisfied: networkx>=2.4 in /opt/anaconda3/lib/python3.9/site-packages (from visions[type_image_path]<0.7.7,>=0.7.5->ydata_profiling) (3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=3.6->typeguard<5,>=3->ydata_profiling) (3.11.0)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.9/site-packages (from patsy>=0.5.2->statsmodels<1,>=0.13.2->ydata_profiling) (1.16.0)\n",
      "Downloading ydata_profiling-4.8.3-py2.py3-none-any.whl (359 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m359.5/359.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ImageHash-4.3.1-py2.py3-none-any.whl (296 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.5/296.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dacite-1.8.1-py3-none-any.whl (14 kB)\n",
      "Downloading multimethod-1.11.2-py3-none-any.whl (10 kB)\n",
      "Downloading phik-0.12.4-cp39-cp39-macosx_10_13_x86_64.whl (659 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m659.3/659.3 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.7.1-py3-none-any.whl (409 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.3/409.3 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.18.2-cp39-cp39-macosx_10_12_x86_64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typeguard-4.2.1-py3-none-any.whl (34 kB)\n",
      "Downloading wordcloud-1.9.3-cp39-cp39-macosx_10_9_x86_64.whl (173 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Using cached typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Downloading visions-0.7.6-py3-none-any.whl (104 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.8/104.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: htmlmin\n",
      "  Building wheel for htmlmin (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27081 sha256=0613c95249a0bf0f1bc22fc19dad45230ce9bbb66971fbe020f7e6271bda7d0d\n",
      "  Stored in directory: /Users/abhiram/Library/Caches/pip/wheels/1d/05/04/c6d7d3b66539d9e659ac6dfe81e2d0fd4c1a8316cc5a403300\n",
      "Successfully built htmlmin\n",
      "Installing collected packages: htmlmin, typing-extensions, multimethod, dacite, annotated-types, typeguard, pydantic-core, imagehash, wordcloud, visions, pydantic, phik, ydata_profiling\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.7.1\n",
      "    Uninstalling typing_extensions-4.7.1:\n",
      "      Successfully uninstalled typing_extensions-4.7.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.4.3 requires pyqt5<5.16, which is not installed.\n",
      "spyder 5.4.3 requires pyqtwebengine<5.16, which is not installed.\n",
      "python-lsp-black 1.2.1 requires black>=22.3.0, but you have black 0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed annotated-types-0.6.0 dacite-1.8.1 htmlmin-0.1.12 imagehash-4.3.1 multimethod-1.11.2 phik-0.12.4 pydantic-2.7.1 pydantic-core-2.18.2 typeguard-4.2.1 typing-extensions-4.11.0 visions-0.7.6 wordcloud-1.9.3 ydata_profiling-4.8.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ydata_profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e35e22fb-68db-4e41-9679-c790fc856e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from ydata_profiling import ProfileReport\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler  # Import StandardScaler\n",
    "\n",
    "\n",
    "conn = sqlite3.connect('group_project.db')#create a connection to the database\n",
    "\n",
    "#defining a sql query to retrieve the data from the 'Housing' table and join with the 'city' and 'statezip' table\n",
    "query = '''\n",
    "        SELECT h.date,h.price, h.bedrooms, h.bathrooms, h.sqft_living, h.sqft_lot, h.floors,\n",
    "        h.waterfront, h.view, h.condition, h.sqft_above, h.sqft_basement, h.yr_built,\n",
    "        h.yr_renovated, h.country, c.name AS city, s.code AS statezip\n",
    "        FROM  housing h\n",
    "        JOIN city c ON h.city_id = c.id\n",
    "        JOIN statezip s ON h.statezip_id = s.id;\n",
    "'''\n",
    "df = pd.read_sql_query(query, conn)#executing the sql query and load the results in the pandas dataframe\n",
    "\n",
    "\n",
    "conn.close()#close the coonnection to the database\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ef82207-3bad-4fee-9aa3-965e2224a846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>statezip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [date, price, bedrooms, bathrooms, sqft_living, sqft_lot, floors, waterfront, view, condition, sqft_above, sqft_basement, yr_built, yr_renovated, country, city, statezip]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47bf1d43-65ab-42a1-bc20-1809b8e96a31",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "DataFrame is empty. Pleaseprovide a non-empty DataFrame.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mydata_profiling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ProfileReport\n\u001b[0;32m----> 2\u001b[0m profile \u001b[38;5;241m=\u001b[39m \u001b[43mProfileReport\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mProfiling Report\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m profile\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/ydata_profiling/profile_report.py:110\u001b[0m, in \u001b[0;36mProfileReport.__init__\u001b[0;34m(self, df, minimal, tsmode, sortby, sensitive, explorative, dark_mode, orange_mode, sample, config_file, lazy, typeset, summarizer, config, type_schema, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     69\u001b[0m     df: Optional[Union[pd\u001b[38;5;241m.\u001b[39mDataFrame, sDataFrame]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     85\u001b[0m ):\n\u001b[1;32m     86\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate a ProfileReport based on a pandas or spark.sql DataFrame\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m    Config processing order (in case of duplicate entries, entries later in the order are retained):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m        **kwargs: other arguments, for valid arguments, check the default configuration file.\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__validate_inputs(df, minimal, tsmode, config_file, lazy)\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config_file \u001b[38;5;129;01mor\u001b[39;00m minimal:\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config_file:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/ydata_profiling/profile_report.py:186\u001b[0m, in \u001b[0;36mProfileReport.__validate_inputs\u001b[0;34m(df, minimal, tsmode, config_file, lazy)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(df, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m--> 186\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    187\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame is empty. Please\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprovide a non-empty DataFrame.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    188\u001b[0m         )\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tsmode:\n",
      "\u001b[0;31mValueError\u001b[0m: DataFrame is empty. Pleaseprovide a non-empty DataFrame."
     ]
    }
   ],
   "source": [
    "#import profile report\n",
    "from ydata_profiling import ProfileReport\n",
    "#generate a profile report for the dataframe 'df' using ProfileReport class\n",
    "profile = ProfileReport(df, title=\"Profiling Report\")\n",
    "profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3d4e6f2-50af-4fc9-bb5c-8416a89e40f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m      4\u001b[0m     plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m      5\u001b[0m     sns\u001b[38;5;241m.\u001b[39mhistplot(train[column], kde\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "#importing the matplotlib library\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline#to ensure plots are displayed inline \n",
    "for column in train.columns:#iterating through each column in the training data\n",
    "    plt.figure(figsize=(8, 6))#creating a new figure for each columns histogram\n",
    "    sns.histplot(train[column], kde=True)#plot a histogram with KDE using seaborn\n",
    "    plt.title(f'Distribution of {column}')#setting the title of the plot\n",
    "    plt.tight_layout()#adjust layout to prevent overlapping of the subplots\n",
    "    plt.show()#display the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "28a9d152-28ce-4474-a587-a35110184c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split#importing the necessary metrics from sklearn\n",
    "#splitting the data into train and test data into 80:20\n",
    "#distribution of target variable price\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42, stratify=pd.qcut(df['price'], q=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "de9024e5-c2a9-4890-a4cc-1c60934efe49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Shoreline' 'Seattle' 'Kent' 'Bellevue' 'Redmond' 'Maple Valley'\n",
      " 'North Bend' 'Lake Forest Park' 'Sammamish' 'Auburn' 'Des Moines'\n",
      " 'Bothell' 'Federal Way' 'Kirkland' 'Issaquah' 'Woodinville'\n",
      " 'Normandy Park' 'Fall City' 'Renton' 'Carnation' 'Snoqualmie' 'Duvall'\n",
      " 'Burien' 'Covington' 'Inglewood-Finn Hill' 'Kenmore' 'Newcastle'\n",
      " 'Mercer Island' 'Black Diamond' 'Ravensdale' 'Clyde Hill' 'Algona'\n",
      " 'Skykomish' 'Tukwila' 'Vashon' 'Yarrow Point' 'SeaTac' 'Medina'\n",
      " 'Enumclaw' 'Snoqualmie Pass' 'Pacific' 'Beaux Arts Village' 'Preston'\n",
      " 'Milton']\n",
      "object\n",
      "            date      price  bedrooms  bathrooms  sqft_living  sqft_lot  \\\n",
      "0  5/2/2014 0:00   313000.0         3       1.50         1340      7912   \n",
      "1  5/2/2014 0:00  2384000.0         5       2.50         3650      9050   \n",
      "2  5/2/2014 0:00   342000.0         3       2.00         1930     11947   \n",
      "3  5/2/2014 0:00   420000.0         3       2.25         2000      8030   \n",
      "4  5/2/2014 0:00   550000.0         4       2.50         1940     10500   \n",
      "\n",
      "   floors  waterfront  view  condition  ...  city_SeaTac  city_Seattle  \\\n",
      "0     1.5           0     0          3  ...          0.0           0.0   \n",
      "1     2.0           0     4          5  ...          0.0           1.0   \n",
      "2     1.0           0     0          4  ...          0.0           0.0   \n",
      "3     1.0           0     0          4  ...          0.0           0.0   \n",
      "4     1.0           0     0          4  ...          0.0           0.0   \n",
      "\n",
      "   city_Shoreline  city_Skykomish city_Snoqualmie  city_Snoqualmie Pass  \\\n",
      "0             1.0             0.0             0.0                   0.0   \n",
      "1             0.0             0.0             0.0                   0.0   \n",
      "2             0.0             0.0             0.0                   0.0   \n",
      "3             0.0             0.0             0.0                   0.0   \n",
      "4             0.0             0.0             0.0                   0.0   \n",
      "\n",
      "   city_Tukwila  city_Vashon  city_Woodinville  city_Yarrow Point  \n",
      "0           0.0          0.0               0.0                0.0  \n",
      "1           0.0          0.0               0.0                0.0  \n",
      "2           0.0          0.0               0.0                0.0  \n",
      "3           0.0          0.0               0.0                0.0  \n",
      "4           0.0          0.0               0.0                0.0  \n",
      "\n",
      "[5 rows x 59 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ideal\\AppData\\Local\\Temp\\ipykernel_37896\\1943669450.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['city'].fillna('Unknown', inplace=True)\n",
      "C:\\Users\\ideal\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('property.csv')\n",
    "\n",
    "# Inspect the unique values and data types in the 'street' column\n",
    "print(df['city'].unique())\n",
    "print(df['city'].dtype)\n",
    "\n",
    "# Handle missing values if any\n",
    "df['city'].fillna('Unknown', inplace=True)\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "# Fit and transform the 'street' column\n",
    "city_encoded = onehot_encoder.fit_transform(df[['city']])\n",
    "\n",
    "# Create a DataFrame with the encoded features\n",
    "city_encoded_df = pd.DataFrame(city_encoded, columns=onehot_encoder.get_feature_names_out(['city']))\n",
    "\n",
    "# Concatenate the encoded features with the original DataFrame\n",
    "df = pd.concat([df, city_encoded_df], axis=1)\n",
    "\n",
    "# Drop the original 'city' column\n",
    "df.drop('city', axis=1, inplace=True)\n",
    "\n",
    "# Display the DataFrame with the encoded 'street' column\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "15e8d02c-91af-4c62-a8f3-08349a3f14c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['date'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[202], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m test\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m,  axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5446\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdrop(\n\u001b[0;32m   5582\u001b[0m         labels\u001b[38;5;241m=\u001b[39mlabels,\n\u001b[0;32m   5583\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   5584\u001b[0m         index\u001b[38;5;241m=\u001b[39mindex,\n\u001b[0;32m   5585\u001b[0m         columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   5586\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[0;32m   5587\u001b[0m         inplace\u001b[38;5;241m=\u001b[39minplace,\n\u001b[0;32m   5588\u001b[0m         errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   5589\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_drop_axis(labels, axis, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['date'] not found in axis\""
     ]
    }
   ],
   "source": [
    "train.drop('date', axis=1, inplace=True)#dropping the date column from the training data\n",
    "test.drop('date',  axis=1, inplace=True)#dropping the date column from the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "cd196782-bb77-49bc-9551-6711452ed814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n",
       "       'waterfront', 'view', 'condition', 'sqft_above', 'sqft_basement',\n",
       "       'yr_built', 'yr_renovated', 'city_Algona', 'city_Auburn',\n",
       "       'city_Beaux Arts Village', 'city_Bellevue', 'city_Black Diamond',\n",
       "       'city_Bothell', 'city_Burien', 'city_Carnation', 'city_Clyde Hill',\n",
       "       'city_Covington', 'city_Des Moines', 'city_Duvall', 'city_Enumclaw',\n",
       "       'city_Fall City', 'city_Federal Way', 'city_Inglewood-Finn Hill',\n",
       "       'city_Issaquah', 'city_Kenmore', 'city_Kent', 'city_Kirkland',\n",
       "       'city_Lake Forest Park', 'city_Maple Valley', 'city_Medina',\n",
       "       'city_Mercer Island', 'city_Milton', 'city_Newcastle',\n",
       "       'city_Normandy Park', 'city_North Bend', 'city_Pacific', 'city_Preston',\n",
       "       'city_Ravensdale', 'city_Redmond', 'city_Renton', 'city_Sammamish',\n",
       "       'city_SeaTac', 'city_Seattle', 'city_Shoreline', 'city_Skykomish',\n",
       "       'city_Snoqualmie', 'city_Snoqualmie Pass', 'city_Tukwila',\n",
       "       'city_Vashon', 'city_Woodinville', 'city_Yarrow Point'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "e0b531f2-f5b6-4571-8ed8-8ce42b3d4ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n",
       "       'waterfront', 'view', 'condition', 'sqft_above', 'sqft_basement',\n",
       "       'yr_built', 'yr_renovated', 'city_Algona', 'city_Auburn',\n",
       "       'city_Beaux Arts Village', 'city_Bellevue', 'city_Black Diamond',\n",
       "       'city_Bothell', 'city_Burien', 'city_Carnation', 'city_Clyde Hill',\n",
       "       'city_Covington', 'city_Des Moines', 'city_Duvall', 'city_Enumclaw',\n",
       "       'city_Fall City', 'city_Federal Way', 'city_Inglewood-Finn Hill',\n",
       "       'city_Issaquah', 'city_Kenmore', 'city_Kent', 'city_Kirkland',\n",
       "       'city_Lake Forest Park', 'city_Maple Valley', 'city_Medina',\n",
       "       'city_Mercer Island', 'city_Milton', 'city_Newcastle',\n",
       "       'city_Normandy Park', 'city_North Bend', 'city_Pacific', 'city_Preston',\n",
       "       'city_Ravensdale', 'city_Redmond', 'city_Renton', 'city_Sammamish',\n",
       "       'city_SeaTac', 'city_Seattle', 'city_Shoreline', 'city_Skykomish',\n",
       "       'city_Snoqualmie', 'city_Snoqualmie Pass', 'city_Tukwila',\n",
       "       'city_Vashon', 'city_Woodinville', 'city_Yarrow Point'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "94b03265-eadd-479e-8d8d-4e810f5cffd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary metrics\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DataPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    categorical_columns = train.select_dtypes(include=['object']).columns\n",
    "    numerical_columns = train.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "    def fit(self, X, y=None): \n",
    "        # Create and fit simple imputer\n",
    "        self.imputer = SimpleImputer(strategy='median')\n",
    "        self.imputer.fit(X[self.numerical_columns])\n",
    "        \n",
    "        # Create and fit Standard Scaler \n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaler.fit(X[self.numerical_columns]) \n",
    "        \n",
    "        # Create and fit one hot encoder\n",
    "        self.onehot = OneHotEncoder(handle_unknown='ignore')\n",
    "        self.onehot.fit(X[self.categorical_columns])\n",
    "        \n",
    "        return self \n",
    " \n",
    "\n",
    "    def transform(self, X): \n",
    "        # Apply simple imputer \n",
    "        imputed_cols = self.imputer.transform(X[self.numerical_columns])\n",
    "        onehot_cols = self.onehot.transform(X[self.categorical_columns])\n",
    "        \n",
    "        # Copy the df \n",
    "        transformed_df = X.copy()\n",
    "         \n",
    "        # Apply transformed columns\n",
    "        transformed_df[self.numerical_columns] = imputed_cols\n",
    "        transformed_df[self.numerical_columns] = self.scaler.transform(transformed_df[self.numerical_columns])        \n",
    "        \n",
    "        # Drop existing categorical columns and replace with one hot equivalent\n",
    "        transformed_df = transformed_df.drop(self.categorical_columns, axis=1) \n",
    "        transformed_df[self.onehot.get_feature_names_out()] = onehot_cols.toarray().astype(int)\n",
    "        \n",
    "        return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "d45f7c37-1229-47c1-92f7-bbf6d620a303",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['date', 'country'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[200], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m DataPreprocessor()\n\u001b[1;32m----> 2\u001b[0m preprocessor\u001b[38;5;241m.\u001b[39mfit(train)\n\u001b[0;32m      3\u001b[0m train_fixed \u001b[38;5;241m=\u001b[39m preprocessor\u001b[38;5;241m.\u001b[39mtransform(train)\n",
      "Cell \u001b[1;32mIn[150], line 20\u001b[0m, in \u001b[0;36mDataPreprocessor.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Create and fit one hot encoder\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monehot \u001b[38;5;241m=\u001b[39m OneHotEncoder(handle_unknown\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39monehot\u001b[38;5;241m.\u001b[39mfit(X[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategorical_columns])\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['date', 'country'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "preprocessor = DataPreprocessor()#create an instance of DataPreprocessor class to handle preprocessing tasks\n",
    "preprocessor.fit(train)#fit the preprocessor to the training data\n",
    "train_fixed = preprocessor.transform(train)#transform the training data using the fitted preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "37cfff65-2162-4ac4-af61-be4552a30561",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#creating a pipeline that includes data preprocessing using Data preprocessor\n",
    "rfr = make_pipeline(DataPreprocessor(), RandomForestRegressor(n_estimators=50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "01eb0a12-49ae-42d1-8035-2cc5d858ae51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('datapreprocessor', DataPreprocessor()),\n",
       "  ('randomforestregressor', RandomForestRegressor(n_estimators=50))],\n",
       " 'verbose': False,\n",
       " 'datapreprocessor': DataPreprocessor(),\n",
       " 'randomforestregressor': RandomForestRegressor(n_estimators=50),\n",
       " 'randomforestregressor__bootstrap': True,\n",
       " 'randomforestregressor__ccp_alpha': 0.0,\n",
       " 'randomforestregressor__criterion': 'squared_error',\n",
       " 'randomforestregressor__max_depth': None,\n",
       " 'randomforestregressor__max_features': 1.0,\n",
       " 'randomforestregressor__max_leaf_nodes': None,\n",
       " 'randomforestregressor__max_samples': None,\n",
       " 'randomforestregressor__min_impurity_decrease': 0.0,\n",
       " 'randomforestregressor__min_samples_leaf': 1,\n",
       " 'randomforestregressor__min_samples_split': 2,\n",
       " 'randomforestregressor__min_weight_fraction_leaf': 0.0,\n",
       " 'randomforestregressor__n_estimators': 50,\n",
       " 'randomforestregressor__n_jobs': None,\n",
       " 'randomforestregressor__oob_score': False,\n",
       " 'randomforestregressor__random_state': None,\n",
       " 'randomforestregressor__verbose': 0,\n",
       " 'randomforestregressor__warm_start': False}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = rfr.get_params()#retrieve the parameters from the random forest pipeline\n",
    "params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "7f292d0a-050a-4aba-be90-f7a7cc8fbe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating the target variable price from the features in training data\n",
    "y_train = train[\"price\"]#target variable\n",
    "X_train = train.drop(\"price\",axis=1)#droppimg the price column\n",
    "y_test = test['price']#target variable\n",
    "X_test = test.drop('price', axis=1)#droppimg the price column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "2c376f2d-ce23-4776-bc7b-b48abdeb2f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr = RandomForestRegressor()#initializing a random forest regressor model\n",
    "rfr.fit(X_train, y_train)#fit the model to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "1332055f-7801-4b6d-b66e-866516117a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "y_train_hat=rfr.predict(X_train)#predict the target variable using the trained RandomForest Regressor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "87dc9785-33ed-49d9-b3d8-7070f561cb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = mean_squared_error(y_train, y_train_hat)#caluculate the MSE between actual and predicted target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "6889c358-2eeb-42d1-a0b8-58d791c6fd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae = mean_absolute_error(y_train, y_train_hat)#caluculate MAE between actual and predicted target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "f43703d5-b369-48f6-be43-d824614ec4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = r2_score(y_train, y_train_hat)#caluculate the r2 error between actual and predicted target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0bef43f9-af4d-49e0-b6c8-5b5431f3a015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors',\n",
       "       'waterfront', 'view', 'condition', 'sqft_above', 'sqft_basement',\n",
       "       'yr_built', 'yr_renovated', 'country', 'city_Algona', 'city_Auburn',\n",
       "       'city_Beaux Arts Village', 'city_Bellevue', 'city_Black Diamond',\n",
       "       'city_Bothell', 'city_Burien', 'city_Carnation', 'city_Clyde Hill',\n",
       "       'city_Covington', 'city_Des Moines', 'city_Duvall', 'city_Enumclaw',\n",
       "       'city_Fall City', 'city_Federal Way', 'city_Inglewood-Finn Hill',\n",
       "       'city_Issaquah', 'city_Kenmore', 'city_Kent', 'city_Kirkland',\n",
       "       'city_Lake Forest Park', 'city_Maple Valley', 'city_Medina',\n",
       "       'city_Mercer Island', 'city_Milton', 'city_Newcastle',\n",
       "       'city_Normandy Park', 'city_North Bend', 'city_Pacific', 'city_Preston',\n",
       "       'city_Ravensdale', 'city_Redmond', 'city_Renton', 'city_Sammamish',\n",
       "       'city_SeaTac', 'city_Seattle', 'city_Shoreline', 'city_Skykomish',\n",
       "       'city_Snoqualmie', 'city_Snoqualmie Pass', 'city_Tukwila',\n",
       "       'city_Vashon', 'city_Woodinville', 'city_Yarrow Point'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3d19a17b-d3d0-4d62-957f-8db8ce78f536",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop('country', axis=1, inplace=True)#dropping the country column from the training data\n",
    "test.drop('country',  axis=1, inplace=True)#dropping the country column from the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "0c823dd2-6420-4c15-a86c-c9035cebb161",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_37896\\3273540463.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6295\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6296\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6297\u001b[0m         ):\n\u001b[0;32m   6298\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6299\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "y_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "230ee1c4-6386-4383-a785-89bd6af3789d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ideal\\anaconda3\\Lib\\site-packages\\mlflow\\types\\utils.py:394: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ideal\\anaconda3\\Lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Successfully registered model 'rfr_housing_model'.\n",
      "2024/05/15 22:59:02 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: rfr_housing_model, version 1\n",
      "Created version '1' of model 'rfr_housing_model'.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "# Set our tracking server uri for logging\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")\n",
    "\n",
    "# Create a new MLflow Experiment\n",
    "mlflow.set_experiment(\"Housing Price Prediction\")\n",
    "experiment_name = \"RANDOM_FOREST_REGRESSI0N\"\n",
    "try:\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "except MlflowException as e:\n",
    "    if \"RESOURCE_DOES_NOT_EXIST\" in str(e):\n",
    "        mlflow.create_experiment(experiment_name)\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "    else:\n",
    "        raise e\n",
    "# Start an MLflow run\n",
    "\n",
    "model = RandomForestRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Log the hyperparameters\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"root_mean_squared_error\", rmse)\n",
    "    mlflow.log_metric(\"mean_absolute_error\", mae)\n",
    "    mlflow.log_metric(\"r2_score\", r2)\n",
    "\n",
    "    # Set a tag that we can use to remind ourselves what this run was for\n",
    "    mlflow.set_tag(\"Model Info\", \"RandomForestRegressor for housing data\")\n",
    "\n",
    "    # Infer the model signature\n",
    "    signature = infer_signature(X_train, model.predict(X_train))\n",
    "\n",
    "    # Log the model\n",
    "    model_info = mlflow.sklearn.log_model(\n",
    "        sk_model=model,\n",
    "        artifact_path=\"housing_model\",\n",
    "        signature=signature,\n",
    "        registered_model_name=\"rfr_housing_model\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "d1717a2e-cd15-4e97-881f-c5caebe5d404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-2.0.3-py3-none-win_amd64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\ideal\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\ideal\\anaconda3\\lib\\site-packages (from xgboost) (1.11.4)\n",
      "Downloading xgboost-2.0.3-py3-none-win_amd64.whl (99.8 MB)\n",
      "   ---------------------------------------- 0.0/99.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/99.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/99.8 MB 217.9 kB/s eta 0:07:38\n",
      "   ---------------------------------------- 0.1/99.8 MB 469.7 kB/s eta 0:03:33\n",
      "   ---------------------------------------- 0.3/99.8 MB 1.9 MB/s eta 0:00:53\n",
      "   ---------------------------------------- 1.2/99.8 MB 5.2 MB/s eta 0:00:19\n",
      "   - -------------------------------------- 2.5/99.8 MB 10.0 MB/s eta 0:00:10\n",
      "   - -------------------------------------- 4.1/99.8 MB 13.8 MB/s eta 0:00:07\n",
      "   -- ------------------------------------- 6.0/99.8 MB 17.5 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 8.1/99.8 MB 20.0 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 10.0/99.8 MB 21.9 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 11.8/99.8 MB 38.5 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 14.2/99.8 MB 40.9 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 15.4/99.8 MB 40.9 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 17.6/99.8 MB 43.7 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 19.2/99.8 MB 43.7 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 22.0/99.8 MB 46.7 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 23.6/99.8 MB 43.5 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 25.2/99.8 MB 40.9 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 27.0/99.8 MB 40.9 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 28.8/99.8 MB 40.9 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 30.2/99.8 MB 38.6 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 32.1/99.8 MB 36.4 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 33.4/99.8 MB 34.4 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 35.1/99.8 MB 36.4 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 36.4/99.8 MB 36.4 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 38.2/99.8 MB 34.4 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 40.2/99.8 MB 34.6 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 42.2/99.8 MB 38.5 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 44.2/99.8 MB 38.6 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 46.0/99.8 MB 40.9 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 47.7/99.8 MB 38.5 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 48.9/99.8 MB 38.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 50.7/99.8 MB 38.5 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 52.7/99.8 MB 38.5 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 54.2/99.8 MB 36.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 56.3/99.8 MB 38.5 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 57.9/99.8 MB 38.5 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 59.7/99.8 MB 38.6 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 60.8/99.8 MB 40.9 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 62.1/99.8 MB 34.4 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 63.6/99.8 MB 32.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 65.0/99.8 MB 32.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 66.0/99.8 MB 32.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 67.1/99.8 MB 31.2 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 68.2/99.8 MB 28.5 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 69.3/99.8 MB 26.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 70.2/99.8 MB 27.3 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 70.8/99.8 MB 24.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 71.6/99.8 MB 24.3 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 72.9/99.8 MB 23.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 73.4/99.8 MB 23.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 73.9/99.8 MB 20.5 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 74.9/99.8 MB 19.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 75.8/99.8 MB 19.3 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 77.0/99.8 MB 19.8 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 77.9/99.8 MB 19.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 79.1/99.8 MB 19.3 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 80.4/99.8 MB 18.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 80.4/99.8 MB 18.7 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 81.1/99.8 MB 18.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 82.6/99.8 MB 19.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 84.1/99.8 MB 21.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 85.5/99.8 MB 23.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 87.4/99.8 MB 24.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 89.2/99.8 MB 27.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 91.1/99.8 MB 32.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 92.7/99.8 MB 34.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 93.8/99.8 MB 34.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 95.6/99.8 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  97.4/99.8 MB 36.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  98.4/99.8 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  99.7/99.8 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 99.8/99.8 MB 14.9 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-2.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "f4045c16-5797-4b20-8171-a8c951283cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ideal\\anaconda3\\Lib\\site-packages\\mlflow\\types\\utils.py:394: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ideal\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [23:26:16] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\c_api\\c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "Successfully registered model 'xgb_housing_model'.\n",
      "2024/05/15 23:26:20 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: xgb_housing_model, version 1\n",
      "Created version '1' of model 'xgb_housing_model'.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Set our tracking server uri for logging\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")\n",
    "\n",
    "# Create a new MLflow Experiment\n",
    "experiment_name = \"Housing Price Prediction\"\n",
    "try:\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "except mlflow.exceptions.MlflowException as e:\n",
    "    if \"RESOURCE_DOES_NOT_EXIST\" in str(e):\n",
    "        mlflow.create_experiment(experiment_name)\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "# Define the model\n",
    "model = XGBRegressor()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    # Log the hyperparameters\n",
    "    mlflow.log_params(model.get_params())\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate and log metrics\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    mlflow.log_metric(\"root_mean_squared_error\", rmse)\n",
    "    mlflow.log_metric(\"mean_absolute_error\", mae)\n",
    "    mlflow.log_metric(\"r2_score\", r2)\n",
    "\n",
    "    # Set a tag that we can use to remind ourselves what this run was for\n",
    "    mlflow.set_tag(\"Model Info\", \"XGBRegressor for housing data\")\n",
    "\n",
    "    # Infer the model signature\n",
    "    signature = infer_signature(X_train, model.predict(X_train))\n",
    "\n",
    "    # Log the model\n",
    "    model_info = mlflow.xgboost.log_model(\n",
    "        xgb_model=model,\n",
    "        artifact_path=\"housing_model\",\n",
    "        signature=signature,\n",
    "        registered_model_name=\"xgb_housing_model\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "d8df409b-c1f2-40fd-87f0-f6d790fef5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ideal\\anaconda3\\Lib\\site-packages\\mlflow\\types\\utils.py:394: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "Successfully registered model 'bagging_housing_model'.\n",
      "2024/05/15 23:40:13 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: bagging_housing_model, version 1\n",
      "Created version '1' of model 'bagging_housing_model'.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Set our tracking server uri for logging\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")\n",
    "\n",
    "# Create a new MLflow Experiment\n",
    "experiment_name = \"Housing Price Prediction\"\n",
    "try:\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "except mlflow.exceptions.MlflowException as e:\n",
    "    if \"RESOURCE_DOES_NOT_EXIST\" in str(e):\n",
    "        mlflow.create_experiment(experiment_name)\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "# Define the model using BaggingRegressor\n",
    "model = BaggingRegressor()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    # Log the hyperparameters\n",
    "    mlflow.log_params(model.get_params())\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate and log metrics\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    mlflow.log_metric(\"root_mean_squared_error\", rmse)\n",
    "    mlflow.log_metric(\"mean_absolute_error\", mae)\n",
    "    mlflow.log_metric(\"r2_score\", r2)\n",
    "\n",
    "    # Set a tag that we can use to remind ourselves what this run was for\n",
    "    mlflow.set_tag(\"Model Info\", \"BaggingRegressor for housing data\")\n",
    "\n",
    "    # Infer the model signature\n",
    "    signature = infer_signature(X_train, model.predict(X_train))\n",
    "\n",
    "    # Log the model\n",
    "    model_info = mlflow.sklearn.log_model(\n",
    "        sk_model=model,\n",
    "        artifact_path=\"housing_model\",\n",
    "        signature=signature,\n",
    "        registered_model_name=\"bagging_housing_model\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "a1f1acc8-a6ab-4f2a-a76d-068f434a3040",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop('view', axis=1, inplace=True)#dropping the view column from the training data\n",
    "test.drop('view',  axis=1, inplace=True)#dropping the view column from the test data\n",
    "train.drop('waterfront', axis=1, inplace=True)#dropping the waterfront column from the training data\n",
    "test.drop('waterfront',  axis=1, inplace=True)#dropping the waterfront column from the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "0a5a61aa-8069-4e7e-8230-5ac55489bbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ideal\\anaconda3\\Lib\\site-packages\\mlflow\\types\\utils.py:394: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "Successfully registered model 'gbr_housing_model'.\n",
      "2024/05/15 23:47:04 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: gbr_housing_model, version 1\n",
      "Created version '1' of model 'gbr_housing_model'.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Set our tracking server uri for logging\n",
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")\n",
    "\n",
    "# Create a new MLflow Experiment\n",
    "experiment_name = \"Housing Price Prediction\"\n",
    "try:\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "except mlflow.exceptions.MlflowException as e:\n",
    "    if \"RESOURCE_DOES_NOT_EXIST\" in str(e):\n",
    "        mlflow.create_experiment(experiment_name)\n",
    "        mlflow.set_experiment(experiment_name)\n",
    "    else:\n",
    "        raise e\n",
    "\n",
    "# Define the model using GradientBoostingRegressor\n",
    "model = GradientBoostingRegressor()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run():\n",
    "    # Log the hyperparameters\n",
    "    mlflow.log_params(model.get_params())\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate and log metrics\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    mlflow.log_metric(\"root_mean_squared_error\", rmse)\n",
    "    mlflow.log_metric(\"mean_absolute_error\", mae)\n",
    "    mlflow.log_metric(\"r2_score\", r2)\n",
    "\n",
    "    # Set a tag that we can use to remind ourselves what this run was for\n",
    "    mlflow.set_tag(\"Model Info\", \"GradientBoostingRegressor for housing data\")\n",
    "\n",
    "    # Infer the model signature\n",
    "    signature = infer_signature(X_train, model.predict(X_train))\n",
    "\n",
    "    # Log the model\n",
    "    model_info = mlflow.sklearn.log_model(\n",
    "        sk_model=model,\n",
    "        artifact_path=\"housing_model\",\n",
    "        signature=signature,\n",
    "        registered_model_name=\"gbr_housing_model\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0f891b-5876-47b3-97e0-ed8b5c51efc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
